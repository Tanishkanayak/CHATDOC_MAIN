# ğŸ“„ Document Chatbot with Retrieval Augmented Generation (RAG)

Welcome to the **Document Chatbot with Retrieval Augmented Generation (RAG)** repository! This project enables interactive document retrieval using cutting-edge AI techniques. By integrating the RAG methodology with a Streamlit-powered interface, it allows seamless querying across multiple document formats.

---

## ğŸ“š Table of Contents

- [âœ¨ Overview](#-overview)
- [ğŸ—‚ï¸ File Structure](#ï¸-file-structure)
- [ğŸš€ Features](#-features)
- [ğŸ“‹ Requirements](#-requirements)
- [ğŸ’» Installation](#-installation)

---

## âœ¨ Overview

This chatbot leverages the RAG framework to enhance information retrieval from documents using natural language queries. It combines retrieval-based and generative AI capabilities for a smooth querying experience, making it ideal for dynamic, knowledge-intensive tasks.

The user-friendly Streamlit interface simplifies interaction, ensuring accessibility for users across various domains.

https://github.com/user-attachments/assets/6f549fe3-5594-4cb2-9e90-d51720fcffbc

---

## ğŸ—‚ï¸ File Structure

```
.
â”œâ”€â”€ icons/                    # Icons for UI (bot, user)
â”‚   â”œâ”€â”€ bot-icon.png
â”‚   â””â”€â”€ user-icon.png
â”œâ”€â”€ src/                      # Source code for the app
â”‚   â”œâ”€â”€ app.py                # Main application logic
â”‚   â”œâ”€â”€ conversation.py       # Chat handling and AI chain integration
â”‚   â”œâ”€â”€ document_utils.py     # Document processing and summarization logic
â”‚   â”œâ”€â”€ htmlTemplates.py      # HTML and CSS templates for UI
â”‚   â”œâ”€â”€ prompt_refiner.py     # Logic for refining prompts with LLM
â”‚   â””â”€â”€ text_processing.py    # Text splitting and vector store generation
â”œâ”€â”€ test/                     # Test files for the app (PDF, TXT)
â”‚   â”œâ”€â”€ test.pdf
â”‚   â””â”€â”€ test.txt
â”œâ”€â”€ .env                      # Store your API keys and sensitive data
â”œâ”€â”€ .gitignore                # Git ignore rules
â”œâ”€â”€ LICENSE                   # Project License
â”œâ”€â”€ README.md                 # Project documentation and setup guide
â”œâ”€â”€ directory_tree.py         # Script for visualizing the directory tree (optional)
â”œâ”€â”€ pyproject.toml            # Project dependencies and configuration
â”œâ”€â”€ setup.sh                  # Setup script for environment and dependencies
â””â”€â”€ uv.lock                   # UV lock file for environment management

```

---

## ğŸš€ Features

- **Multi-File Upload**: Process and query multiple documents in formats like PDF, TXT, CSV, and DOCX.
- **AI-Powered Chat**: Ask natural language questions, and the bot retrieves accurate answers from document content.
- **Document Summarization**: Summarizes content for better context understanding.
- **Prompt Refinement**: Refines user questions for more precise responses. You can **enable or disable** this feature via the toggle, allowing you to improve prompt clarity and conciseness when enabled.
- **Memory Retention**: Tracks conversation history for maintaining context.
- **Interactive UI**: A Streamlit-powered interface for intuitive interactions.
- **Extensibility**: Easily customizable for additional document types and use cases.

---

## ğŸ“‹ Requirements

Ensure the following tools are installed:

- **Python 3.8+**: The version is automatically managed in the `setup.sh` script.
- **Dependencies**: Listed in `pyproject.toml` and installed automatically using the `setup.sh` script with the `uv sync` command.
- **UV**: Install via pip:

   ```bash
   pip install uv
   ```

   For more details, visit the [UV documentation page](https://pypi.org/project/uv/).

- **Python-dotenv**: For managing environment variables. Install via pip:

   ```bash
   pip install python-dotenv
   ```

   Add your API keys to a `.env` file in this format:

   ```plaintext
   OPENAI_API_KEY=your_openai_api_key_here
   HUGGINGFACE_API_KEY=your_huggingface_api_key_here
   ```

- **OpenAI API Key**: Required for GPT-4o mini and embedding models.
- **Hugging Face API Key**: Used to interact with Hugging Face's `facebook/bart-large-cnn` model for text summarization when the local summarizer model fails to load.
- **Bash**: Available on Unix-based systems (Linux/macOS). Windows users can use Git Bash or WSL.

---

## ğŸ’» Installation

Follow these steps to set up your environment and install the necessary packages:

1. **Clone the Repository**  

   Clone the repository to your local machine:

   ```bash
   git clone https://github.com/ErfanShm/ChatDoc.git
   cd ChatDoc
   ```

2. **Run the Setup Script**  

   The `setup.sh` script automates the environment setup, including virtual environment creation, dependency synchronization, and application launch options:

   ```bash
   bash setup.sh
   ```

   Choose from the following options:
   - Sync dependencies
   - Run with Streamlit interface
   - Exit

3. **Access the User Interface**  

   Once running, access the user interface by navigating to [http://localhost:8000](http://localhost:8000) in your browser (usually done automatically).

---

## ğŸ“ Usage

- **Input**: Upload multiple documents for analysis.
- **VectorStore Creation**: Converts documents into vector stores using FAISS and OpenAI embeddings.
- **Memory Integration**: Maintains context with a conversational memory buffer.
- **Text Generation**: Leverages GPT-4o mini Turbo for generating accurate responses.
- **Prompt Refinement**: Toggle this feature to refine and clarify your prompts for more concise, understandable outputs.
- **User Interface**: Streamlit provides an intuitive querying experience.

---

## ğŸ™Œ Acknowledgments

We extend our gratitude to the following tools and libraries that make this project possible:

- [Streamlit](https://streamlit.io/): For powering the intuitive user interface.
- [OpenAI](https://openai.com/): For GPT models and embeddings.
- [Hugging Face](https://huggingface.co/): For the `facebook/bart-large-cnn` summarization model.
- [UV](https://pypi.org/project/uv/): For streamlined environment and dependency management.
- [FAISS](https://github.com/facebookresearch/faiss): For efficient similarity search and clustering.
- [LangChain](https://langchain.com/): For chaining multiple AI models into a cohesive workflow.

---

## ğŸ“œ License

This repository is licensed under the [MIT License](https://opensource.org/licenses/MIT). You are free to use, modify, and distribute the code, provided you include the original copyright notice and license.
